{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d749a561-c8fc-4991-afe9-7113d3c6017c",
   "metadata": {},
   "source": [
    "### Trying out RAG with ollama and chromadb\n",
    "ollama is installed in the python environment venvcrc5 from where this notebook is started.\n",
    "\n",
    "ollama is recommended over hugging face for local experimentation\n",
    "\n",
    "it uses a docker-like syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de73eae-777f-4b14-9ed4-cbdd2cfd4556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage:\n",
      "  ollama [flags]\n",
      "  ollama [command]\n",
      "\n",
      "Available Commands:\n",
      "  serve       Start ollama\n",
      "  create      Create a model from a Modelfile\n",
      "  show        Show information for a model\n",
      "  run         Run a model\n",
      "  stop        Stop a running model\n",
      "  pull        Pull a model from a registry\n",
      "  push        Push a model to a registry\n",
      "  list        List models\n",
      "  ps          List running models\n",
      "  cp          Copy a model\n",
      "  rm          Remove a model\n",
      "  help        Help about any command\n",
      "\n",
      "Flags:\n",
      "  -h, --help      help for ollama\n",
      "  -v, --version   Show version information\n",
      "\n",
      "Use \"ollama [command] --help\" for more information about a command.\n",
      "ollama version is 0.5.7\n",
      "NAME                       ID              SIZE      MODIFIED     \n",
      "deepseek-r1:1.5b           e0979632db5a    1.1 GB    6 days ago      \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    6 days ago      \n",
      "deepseek-r1:7b             755ced02ce7b    4.7 GB    7 days ago      \n",
      "deepseek-r1:14b            c333b7232bdb    9.0 GB    3 weeks ago     \n",
      "gemma3:12b                 6fd036cefda5    8.1 GB    4 months ago    \n",
      "deepseek-r1:latest         0a8c26691023    4.7 GB    5 months ago    \n",
      "llama3.1:latest            46e0c10c039e    4.9 GB    6 months ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama\n",
    "!ollama --version\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "289f0622-759b-4071-bb1e-e83d8cffb282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pull a model from a registry\n",
      "\n",
      "Usage:\n",
      "  ollama pull MODEL [flags]\n",
      "\n",
      "Flags:\n",
      "  -h, --help       help for pull\n",
      "      --insecure   Use an insecure registry\n",
      "\n",
      "Environment Variables:\n",
      "      OLLAMA_HOST                IP Address for the ollama server (default 127.0.0.1:11434)\n"
     ]
    }
   ],
   "source": [
    "!ollama help pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d32e73-ede0-4a86-b35a-7fa55ebf7cfb",
   "metadata": {},
   "source": [
    "#### The pdfreader translates any pdf document to text readable by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59742336-ba28-4ad4-ad9f-b483638d3b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "# my textbook, 5th ed\n",
    "reader = PdfReader(\"/home/mort/LaTeX/new projects/CRC5/main.pdf\")\n",
    "total_pages = len(reader.pages)\n",
    "all_text = \"\"\n",
    "for page_num in range(total_pages):\n",
    "    page = reader.pages[page_num]\n",
    "    all_text += page.extract_text()\n",
    "f = open(\"/home/mort/crc5pdfreader/main.txt\", \"w\")\n",
    "f.write(all_text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a22089-22a2-4f18-aa53-8c8bc911f7f0",
   "metadata": {},
   "source": [
    "#### Here the original LaTeX files are collected instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d7d2e14-d5c0-4b02-8151-7a63f0031b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# Find `.tex` files in LaTeX directory and all subdirectories\n",
    "tex_files = glob.glob('/home/mort/LaTeX/new projects/CRC5/**/chapter[1-9].tex', recursive = True)\n",
    "tex_files.sort()\n",
    "f = open(\"/home/mort/crc5latex/main.txt\", \"w\")\n",
    "for file in tex_files:\n",
    "    g = open(file, \"r\")\n",
    "    content = g.read()\n",
    "    f.write(content)\n",
    "    g.close()\n",
    "f.close()    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd89d240-af18-455f-bb83-1a29deeee246",
   "metadata": {},
   "source": [
    "#### Using crawl4ai to scrape web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05592e6a-c814-44de-83b9-d78259b0fc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. â†’ Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m2\u001b[0m\u001b[36m \u001b[0m\n",
      "Crawled 64 pages in total\n",
      "URL: https://mortcanty.github.io/src/\n",
      "MarkDown: # Mort Canty's Home Page\n",
      "![](https://mortcanty.github.io/src/mortandandrea.jpg) |    \n",
      "  \n",
      "  \n",
      "Dr. Mort Canty   \n",
      "Juelich, Germany   \n",
      "[contact](https://mortcanty.github.io/src/contact.html)  \n",
      "---|---  \n",
      "## [Software](https://mortcanty.github.io/src/software.html) |  ![](https://mortcanty.github.io/src/bookcover4.jpg)  \n",
      "---|---  \n",
      "## [ Blog](http://fwenvi-idl.blogspot.com/)\n",
      "## [ Publications](https://mortcanty.github.io/src/publications.html)\n",
      "## Teaching\n",
      "[HU Berlin](https://mortcanty.github.io/src/hu_berlin.html) ENVI/IDL Course, June 14-17, 2010   \n",
      "[ZFL Bonn](https://mortcanty.github.io/src/zfl_2012.html) Radiometric Normalization Course, March/April 2012   \n",
      "[ZFL Bonn](https://mortcanty.github.io/src/zfl_2013.html), ENVI/IDL Course, November, 2013.  \n",
      "[FZJ Juelich](https://mortcanty.github.io/src/stats2014.html), Statistics Course, February, 2014.  \n",
      "[ZFL Bonn](https://mortcanty.github.io/src/python2015.html), Image analysis with Python, April, 2015.  \n",
      "[ZFL Bonn](https://mortcanty.github.io/src/zfl_2016.html), Multi-spectral and polarimetric SAR Image analysis, April, 2016.  \n",
      "[FZJ Juelich](https://mortcanty.github.io/src/stats2016.html), Statistics Course, November 2016.  \n",
      "[ZFL Bonn](https://github.com/mortcanty/ZFLPython), Python Scripting Course, April 2017.  \n",
      "[ZFL Bonn](https://github.com/mortcanty/CRC4Docker), GEE Workshop, April 10/11 2018.[ (HTML Version)](https://mortcanty.github.io/src/zflgee.html),  \n",
      "[ZFL Bonn](https://mortcanty.github.io/src/ML2019.html), Machine Learning Workshop, March/April 2019.  \n",
      "\n",
      "## Links\n",
      "[Juelich Research Center](http://www.fz-juelich.de)  \n",
      "[ Allan Nielsen's Home Page](http://www2.imm.dtu.dk/~aa/)  \n",
      "[Google Earth Engine](https://developers.google.com/earth-engine/)  \n",
      "[Docker Documentation](https://docs.docker.com/)  \n",
      "[ Michael Galloy's Site](http://michaelgalloy.com/)  \n",
      "[GPULib (CUDA interface for IDL)](http://www.txcorp.com/home/GPULib/)  \n",
      "[ David Fanning's IDL Page](http://idlcoyote.com/documents/programs.php)  \n",
      "[ IDL/ENVI Documentation Center](http://www.exelisvis.com/docs/)  \n",
      "[ Numpy and Scipy Documentation](http://docs.scipy.org/doc/)  \n",
      "[ GDAL/OGR in Python](http://trac.osgeo.org/gdal/wiki/GdalOgrInPython%20)  \n",
      "[ Center for Remote Sensing of Land Surfaces (ZFL-Bonn)](https://www.zfl.uni-bonn.de/)   \n",
      "  \n",
      "  \n",
      "\n",
      "\n",
      "Depth: 0\n",
      "URL: https://mortcanty.github.io/src/ML2019.html\n",
      "MarkDown: ## Machine Learning Course 2019\n",
      "Pull and/or run the container for the first time with  \n",
      "  \n",
      "docker run -d -p 443:8888 -p 6006:6006 -v :/home/myimagery/ --name=crc4 mort/crc4docker   \n",
      "  \n",
      "This maps the host directory my_image_folder to the container directory /home/myimagery/ and runs the container in detached mode.   \n",
      "  \n",
      "Point your browser to http://localhost:443 to see the Jupyter home page and open notebook **zfl2019.ipynb**. In order to use Earth Engine scripts, you must authenticate. From the home page open a local terminal and enter \n",
      "earthengine authenticate \n",
      "and follow the instructions. This adds authentication information to the container and only has to be done once.   \n",
      "  \n",
      "Stop the container with \n",
      "docker stop crc4 \n",
      "Re-start with \n",
      "docker start crc4 \n",
      "#### [ HTML Version of the course Jupyter notebook](https://mortcanty.github.io/src/zfl2019.html)\n",
      "\n",
      "Depth: 1\n",
      "URL: https://mortcanty.github.io/src/software.html\n",
      "MarkDown: ## SOFTWARE FOR THE FIFTH EDITION\n",
      "_[ Image Analysis, Classification and Change Detection in Remote Sensing,   \n",
      "with Algorithms for Python, Fifth revised edition](https://github.com/mortcanty/CRC5Docker)  \n",
      "_Taylor & Francis, CRC Press 2025 \n",
      "## SOFTWARE FOR THE FOURTH EDITION\n",
      "_[ Image Analysis, Classification and Change Detection in Remote Sensing,   \n",
      "with Algorithms for Python, Fourth revised edition](https://www.amazon.com/Analysis-Classification-Change-Detection-Sensing/dp/1138613223/ref=dp_ob_title_bk)  \n",
      "_Taylor & Francis, CRC Press 2019 \n",
      "[Errata for the Fourth Revised Edition](https://mortcanty.github.io/src/errata4.pdf)   \n",
      "  \n",
      "[Documentation (PDF)](https://mortcanty.github.io/src/crc4docs.pdf)   \n",
      "  \n",
      "[Scripts and chapter-by-chapter Jupyter notebooks (GitHub)](https://mortcanty.github.io/CRC4Docker/)   \n",
      "  \n",
      "[Sequential SAR change detection on the GEE](https://mortcanty.github.io/EESARDocker/)   \n",
      "  \n",
      "[Supervised classification of sentinel-1 time series](https://mortcanty.github.io/src/s1class.html)   \n",
      "  \n",
      "JavaScript source code for change detection on the Google Earth Engine Code Editor:   \n",
      "_git clone https://earthengine.googlesource.com/users/mortcanty/changedetection_   \n",
      "  \n",
      "\n",
      "## SOFTWARE FOR THE THIRD EDITION\n",
      "_[ Image Analysis, Classification and Change Detection in Remote Sensing,   \n",
      "with Algorithms for ENVI/IDL and Python, Third revised edition](http://www.amazon.com/Analysis-Classification-Change-Detection-Sensing/dp/1466570377/ref=dp_ob_title_bk)  \n",
      "_Taylor & Francis, CRC Press 2014 \n",
      "[Errata for the Third Revised Edition](https://mortcanty.github.io/src/errata3.pdf)   \n",
      "  \n",
      "ENVI/IDL   \n",
      "Documentation [(PDF)](https://mortcanty.github.io/src/idldoc.pdf) or [IDLDOC](https://mortcanty.github.io/src/idldoc/index.html)  \n",
      "Scripts [(GitHub)](https://mortcanty.github.io/CRCENVI)  \n",
      "Coyote library [ (Website) ](http://www.idlcoyote.com/documents/programs.php#COYOTE_LIBRARY_DOWNLOAD)   \n",
      "  \n",
      "PYTHON   \n",
      "Documentation [(PDF)](https://mortcanty.github.io/src/pythondoc.pdf)  \n",
      "Scripts [(GitHub)](https://mortcanty.github.io/CRCPython)   \n",
      "  \n",
      "DOCKER IMAGES   \n",
      "[Optical/infrared image analysis](https://mortcanty.github.io/CRCDocker), see [ tutorial](https://mortcanty.github.io/src/tutorial.html)   \n",
      "[Polarimetric SAR change detection](https://mortcanty.github.io/SARDocker), see [ tutorial](https://mortcanty.github.io/src/tutorialsar.html)\n",
      "[Home Page](https://mortcanty.github.io/src/index.html)\n",
      "  \n",
      "------------------------------------------------  \n",
      "\n",
      "\n",
      "Depth: 1\n"
     ]
    }
   ],
   "source": [
    "!python3 scripts/testcrawler.py \"https://mortcanty.github.io/src/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252ef2f9-2ced-4769-a465-0948676ee37f",
   "metadata": {},
   "source": [
    "#### The LlamaParse version of the text pdf (parsed from the web API) is stored in /home/mort/crc5llamaparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2ac292-fa28-4fcb-a5dd-002b50a4fc6e",
   "metadata": {},
   "source": [
    "#### Code for preprocessing the RAG supplementary text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3ad0df7-ee8e-46e9-b238-2399293e17a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def readtextfiles(path):\n",
    "    text_contents = {}\n",
    "    directory = os.path.join(path)\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "        \n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "        \n",
    "            text_contents[filename] = content\n",
    "        \n",
    "        return text_contents\n",
    "\n",
    "def chunksplitter(text, chunk_size=512, chunk_overlap=128):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,  # Desired chunk size in tokens\n",
    "        chunk_overlap=chunk_overlap,  # Overlap between chunks\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \"]  # Split by paragraphs, then sentences, then words\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# use the nomic-embed-text model to calculate vector embeddings for all text chunks\n",
    "def getembedding(chunks):\n",
    "    embeds = ollama.embed(model=\"nomic-embed-text\", input=chunks)\n",
    "    return embeds.get('embeddings', [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f159d-9e67-4f56-af9a-2627d26f042b",
   "metadata": {},
   "source": [
    "#### Add the supplementary text to a new database collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c45bcb2-4e75-413c-83ca-2c36046d094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "#chromaclient = chromadb.PersistentClient(path=\"/home/mort/crc5imagery/crc5rag\")\n",
    "chromaclient = chromadb.PersistentClient(path=\"/home/mort/crc5rag\")\n",
    "chromaclient.delete_collection(\"crc5rag\")\n",
    "collection = chromaclient.create_collection(name=\"crc5rag\", metadata={\"hnsw:space\": \"cosine\"}  )\n",
    "\n",
    "# the RAG supplementary data, here using the llamaparse version of the main pdf\n",
    "textdocspath = \"/home/mort/crc5llamaparse\"\n",
    "text_data = readtextfiles(textdocspath)\n",
    "\n",
    "# read, break into chunks, embed and add to the chroma vector database \n",
    "for filename, text in text_data.items():\n",
    "    # default chunk size 512, overlap 128 \n",
    "    chunks = chunksplitter(text, chunk_size=256, chunk_overlap=32)\n",
    "    embeds = getembedding(chunks)\n",
    "    chunknumber = list(range(len(chunks)))\n",
    "    ids = [filename + str(index) for index in chunknumber]\n",
    "    metadatas = [{\"source\": filename} for index in chunknumber]\n",
    "    collection.add(ids=ids, documents=chunks, embeddings=embeds, metadatas=metadatas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c9451e-7588-4db5-a87a-d96ee9a152b5",
   "metadata": {},
   "source": [
    "#### Execute a query with llama3.1 or deepseek-r1 and the supplementary text (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d66a69bd-b4a6-4c28-91fc-f4b0ff5a1ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!ollama pull nomic-embed-text\n",
    "#!ollama pull llama3.1\n",
    "!ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "807bd52a-fa9b-435e-80a0-95c6c5560909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import html\n",
    "import ollama\n",
    "import chromadb\n",
    "\n",
    "# Initialize ChromaDB client and collection\n",
    "#chromaclient = chromadb.PersistentClient(path=\"/home/mort/crc5imagery/crc5rag\")\n",
    "chromaclient = chromadb.PersistentClient(path=\"/home/mort/crc5rag\")\n",
    "collection = chromaclient.get_collection(name=\"crc5rag\")\n",
    "\n",
    "def ragask(query):\n",
    "    # Embed the query\n",
    "    queryembed = ollama.embed(model=\"nomic-embed-text\", input=query)['embeddings']\n",
    "    # Retrieve related documents\n",
    "    relateddocs = '\\n\\n'.join(collection.query(query_embeddings=queryembed, n_results=16)['documents'][0])\n",
    "    # Generate a response\n",
    "    prompt = f\"Answer the question: {query} referring to the following text as a resource: {relateddocs}\"\n",
    "    response = ollama.generate(model=\"deepseek-r1:14b\", prompt=prompt, stream=False)['response']   \n",
    "    # Ensure the response is valid Markdown\n",
    "    return html.escape(response)\n",
    "\n",
    "# Launch Gradio Interface (ChatInterface not appropriate for RAG application!)\n",
    "gr.Interface(fn=ragask,inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
    "             outputs=\"markdown\",\n",
    "             description=\"Ask questions about the book contents\",\n",
    "             title=\"Image Analysis, Classification and Change Detection in Remote Sensing\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155124d-17bd-47dd-85cb-7ff45a7b54b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
